{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> a71e2a8 (improve notebook explanation)
    "# Giving tokens a label - How to categorize tokens\n",
    "\n",
    "\n",
    "The first part of this Notebook contains elements that explain how to label tokens and how the functions work.\n",
    "\n",
<<<<<<< HEAD
    "The second part shows how all tokens are labelled that are used for our delphi language models.3\n",
    "\n",
    "# 1) How to use the token labelling functions"
<<<<<<< HEAD
=======
    "# How to label tokens"
>>>>>>> bf8ef79 (add notebook)
=======
>>>>>>> a71e2a8 (improve notebook explanation)
=======
    "The second part shows how all tokens are labelled that are used for our delphi language models.3\n"
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 90,
=======
   "execution_count": 23,
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
>>>>>>> a71e2a8 (improve notebook explanation)
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "from pprint import pprint \n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import delphi\n",
    "\n",
<<<<<<< HEAD
    "# from delphi.eval import token_labelling"
=======
    "import spacy\n",
    "\n",
    "import token_labelling"
>>>>>>> bf8ef79 (add notebook)
=======
    "from pprint import pprint \n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import delphi\n",
    "\n",
    "# from delphi.eval import token_labelling"
>>>>>>> a71e2a8 (improve notebook explanation)
=======
    "from delphi.eval import token_labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1) How to use the token labelling functions"
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze a simple sentence and receive the respective tokens with their analyzed attributes.  \n",
    "The grammatical/linguistic analysis is done by a model provided by spaCy for the English language."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 23,
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 2,
>>>>>>> a71e2a8 (improve notebook explanation)
=======
   "execution_count": 15,
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter \t PROPN \t nsubj \t PERSON\n",
      "is \t AUX \t ROOT \t \n",
      "a \t DET \t det \t \n",
      "person \t NOUN \t attr \t \n"
     ]
    }
   ],
   "source": [
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc object from a given text\n",
    "doc = nlp(\"Peter is a person\")\n",
    "\n",
    "token = doc[0]\n",
    "for tok in doc:\n",
    "    print(tok,\"\\t\", tok.pos_, \"\\t\", tok.dep_, \"\\t\", tok.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the label for our custom token that we just printed."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 46,
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 8,
>>>>>>> a71e2a8 (improve notebook explanation)
=======
   "execution_count": 5,
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> a71e2a8 (improve notebook explanation)
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
=======
>>>>>>> e0ed3b4 (add the files containing token information/labels)
      "{'Capitalized': True,\n",
      " 'Is Adjective': False,\n",
      " 'Is Adposition': False,\n",
      " 'Is Adverb': False,\n",
      " 'Is Auxiliary': False,\n",
      " 'Is Coordinating conjuction': False,\n",
      " 'Is Determiner': False,\n",
      " 'Is Interjunction': False,\n",
      " 'Is Named Entity': True,\n",
      " 'Is Noun': False,\n",
      " 'Is Numeral': False,\n",
      " 'Is Other': False,\n",
      " 'Is Particle': False,\n",
      " 'Is Pronoun': False,\n",
      " 'Is Proper Noun': True,\n",
      " 'Is Punctuation': False,\n",
      " 'Is Subordinating conjuction': False,\n",
      " 'Is Symbol': False,\n",
      " 'Is Verb': False,\n",
      " 'Starts with space': False}\n"
<<<<<<< HEAD
=======
      "[False, True, False, True, False, False, False, False, False, False, True, False, False, False, False]\n"
>>>>>>> bf8ef79 (add notebook)
=======
>>>>>>> a71e2a8 (improve notebook explanation)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "from delphi.eval import token_labelling\n",
    "\n",
    "label = token_labelling.label_single_token(token)\n",
    "pprint(label)"
=======
    "label = token_labelling.label_single_token(token)\n",
    "print(label)"
>>>>>>> bf8ef79 (add notebook)
=======
    "from delphi.eval import token_labelling\n",
    "\n",
    "label = token_labelling.label_single_token(token)\n",
    "pprint(label)"
>>>>>>> a71e2a8 (improve notebook explanation)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an understanding of what the labels acutally mean.\n",
    "Use this function to receive an explanation for a single token."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 42,
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 9,
>>>>>>> a71e2a8 (improve notebook explanation)
=======
   "execution_count": 6,
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Explanation of token labels --------\n",
      "Token text:          Peter\n",
      "Token dependency:    nominal subject\n",
      "Token POS:           proper noun\n",
      "---------------- Token labels ---------------\n",
      "  0   Starts with space    False\n",
      "  1   Capitalized          True\n",
<<<<<<< HEAD
      "  2   Is Noun              False\n",
      "  3   Is Pronoun           True\n",
      "  4   Is Adjective         False\n",
      "  5   Is Verb              False\n",
      "  6   Is Adverb            False\n",
      "  7   Is Preposition       False\n",
      "  8   Is Conjunction       False\n",
      "  9   Is Interjunction     False\n",
<<<<<<< HEAD
<<<<<<< HEAD
      " 10   Is Named Entity      False\n"
=======
      " 10   Is Subject           True\n",
      " 11   Is Object            False\n",
      " 12   Is Root              False\n",
      " 13   Is auxiliary         False\n",
      " 14   Is Named Entity      False\n"
>>>>>>> bf8ef79 (add notebook)
=======
      " 10   Is Named Entity      False\n"
>>>>>>> a71e2a8 (improve notebook explanation)
=======
      "  2   Is Adjective         False\n",
      "  3   Is Adposition        False\n",
      "  4   Is Adverb            False\n",
      "  5   Is Auxiliary         False\n",
      "  6   Is Coordinating conjuction False\n",
      "  7   Is Determiner        False\n",
      "  8   Is Interjunction     False\n",
      "  9   Is Noun              False\n",
      " 10   Is Numeral           False\n",
      " 11   Is Particle          False\n",
      " 12   Is Pronoun           False\n",
      " 13   Is Proper Noun       True\n",
      " 14   Is Punctuation       False\n",
      " 15   Is Subordinating conjuction False\n",
      " 16   Is Symbol            False\n",
      " 17   Is Verb              False\n",
      " 18   Is Other             False\n",
      " 19   Is Named Entity      True\n"
>>>>>>> e0ed3b4 (add the files containing token information/labels)
     ]
    }
   ],
   "source": [
    "token_labelling.explain_token_labels(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> a71e2a8 (improve notebook explanation)
    "If you are interested in all the possible labels a token can have, that spaCy is capable of assigning, then call the same function but without any argument:\n",
    "```Python\n",
    ">>> token_labelling.explain_token_labels()\n",
    "```"
<<<<<<< HEAD
=======
    "If you are interested in all the possible labels a token can have, that spaCy is capable of assigning, then call the same function but without any argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation of all 302 token labels (POS, dependency, NER, ...):\n",
      "    ADJ        adjective\n",
      "    ADP        adposition\n",
      "    ADV        adverb\n",
      "    AUX        auxiliary\n",
      "    CONJ       conjunction\n",
      "    CCONJ      coordinating conjunction\n",
      "    DET        determiner\n",
      "    INTJ       interjection\n",
      "    NOUN       noun\n",
      "    NUM        numeral\n",
      "    PART       particle\n",
      "    PRON       pronoun\n",
      "    PROPN      proper noun\n",
      "    PUNCT      punctuation\n",
      "    SCONJ      subordinating conjunction\n",
      "    SYM        symbol\n",
      "    VERB       verb\n",
      "    X          other\n",
      "    EOL        end of line\n",
      "    SPACE      space\n",
      "    .          punctuation mark, sentence closer\n",
      "    ,          punctuation mark, comma\n",
      "    -LRB-      left round bracket\n",
      "    -RRB-      right round bracket\n",
      "    ``         opening quotation mark\n",
      "    \"\"         closing quotation mark\n",
      "    ''         closing quotation mark\n",
      "    :          punctuation mark, colon or ellipsis\n",
      "    $          symbol, currency\n",
      "    #          symbol, number sign\n",
      "    AFX        affix\n",
      "    CC         conjunction, coordinating\n",
      "    CD         cardinal number\n",
      "    DT         determiner\n",
      "    EX         existential there\n",
      "    FW         foreign word\n",
      "    HYPH       punctuation mark, hyphen\n",
      "    IN         conjunction, subordinating or preposition\n",
      "    JJ         adjective (English), other noun-modifier (Chinese)\n",
      "    JJR        adjective, comparative\n",
      "    JJS        adjective, superlative\n",
      "    LS         list item marker\n",
      "    MD         verb, modal auxiliary\n",
      "    NIL        missing tag\n",
      "    NN         noun, singular or mass\n",
      "    NNP        noun, proper singular\n",
      "    NNPS       noun, proper plural\n",
      "    NNS        noun, plural\n",
      "    PDT        predeterminer\n",
      "    POS        possessive ending\n",
      "    PRP        pronoun, personal\n",
      "    PRP$       pronoun, possessive\n",
      "    RB         adverb\n",
      "    RBR        adverb, comparative\n",
      "    RBS        adverb, superlative\n",
      "    RP         adverb, particle\n",
      "    TO         infinitival \"to\"\n",
      "    UH         interjection\n",
      "    VB         verb, base form\n",
      "    VBD        verb, past tense\n",
      "    VBG        verb, gerund or present participle\n",
      "    VBN        verb, past participle\n",
      "    VBP        verb, non-3rd person singular present\n",
      "    VBZ        verb, 3rd person singular present\n",
      "    WDT        wh-determiner\n",
      "    WP         wh-pronoun, personal\n",
      "    WP$        wh-pronoun, possessive\n",
      "    WRB        wh-adverb\n",
      "    SP         space (English), sentence-final particle (Chinese)\n",
      "    ADD        email\n",
      "    NFP        superfluous punctuation\n",
      "    GW         additional word in multi-word expression\n",
      "    XX         unknown\n",
      "    BES        auxiliary \"be\"\n",
      "    HVS        forms of \"have\"\n",
      "    _SP        whitespace\n",
      "    $(         other sentence-internal punctuation mark\n",
      "    $,         comma\n",
      "    $.         sentence-final punctuation mark\n",
      "    ADJA       adjective, attributive\n",
      "    ADJD       adjective, adverbial or predicative\n",
      "    APPO       postposition\n",
      "    APPR       preposition; circumposition left\n",
      "    APPRART    preposition with article\n",
      "    APZR       circumposition right\n",
      "    ART        definite or indefinite article\n",
      "    CARD       cardinal number\n",
      "    FM         foreign language material\n",
      "    ITJ        interjection\n",
      "    KOKOM      comparative conjunction\n",
      "    KON        coordinate conjunction\n",
      "    KOUI       subordinate conjunction with \"zu\" and infinitive\n",
      "    KOUS       subordinate conjunction with sentence\n",
      "    NE         proper noun\n",
      "    NNE        proper noun\n",
      "    PAV        pronominal adverb\n",
      "    PROAV      pronominal adverb\n",
      "    PDAT       attributive demonstrative pronoun\n",
      "    PDS        substituting demonstrative pronoun\n",
      "    PIAT       attributive indefinite pronoun without determiner\n",
      "    PIDAT      attributive indefinite pronoun with determiner\n",
      "    PIS        substituting indefinite pronoun\n",
      "    PPER       non-reflexive personal pronoun\n",
      "    PPOSAT     attributive possessive pronoun\n",
      "    PPOSS      substituting possessive pronoun\n",
      "    PRELAT     attributive relative pronoun\n",
      "    PRELS      substituting relative pronoun\n",
      "    PRF        reflexive personal pronoun\n",
      "    PTKA       particle with adjective or adverb\n",
      "    PTKANT     answer particle\n",
      "    PTKNEG     negative particle\n",
      "    PTKVZ      separable verbal particle\n",
      "    PTKZU      \"zu\" before infinitive\n",
      "    PWAT       attributive interrogative pronoun\n",
      "    PWAV       adverbial interrogative or relative pronoun\n",
      "    PWS        substituting interrogative pronoun\n",
      "    TRUNC      word remnant\n",
      "    VAFIN      finite verb, auxiliary\n",
      "    VAIMP      imperative, auxiliary\n",
      "    VAINF      infinitive, auxiliary\n",
      "    VAPP       perfect participle, auxiliary\n",
      "    VMFIN      finite verb, modal\n",
      "    VMINF      infinitive, modal\n",
      "    VMPP       perfect participle, modal\n",
      "    VVFIN      finite verb, full\n",
      "    VVIMP      imperative, full\n",
      "    VVINF      infinitive, full\n",
      "    VVIZU      infinitive with \"zu\", full\n",
      "    VVPP       perfect participle, full\n",
      "    XY         non-word containing non-letter\n",
      "    AD         adverb\n",
      "    AS         aspect marker\n",
      "    BA         把 in ba-construction\n",
      "    CS         subordinating conjunction\n",
      "    DEC        的 in a relative clause\n",
      "    DEG        associative 的\n",
      "    DER        得 in V-de const. and V-de-R\n",
      "    DEV        地 before VP\n",
      "    ETC        for words 等, 等等\n",
      "    IJ         interjection\n",
      "    LB         被 in long bei-const\n",
      "    LC         localizer\n",
      "    M          measure word\n",
      "    MSP        other particle\n",
      "    NR         proper noun\n",
      "    NT         temporal noun\n",
      "    OD         ordinal number\n",
      "    ON         onomatopoeia\n",
      "    P          preposition excluding 把 and 被\n",
      "    PN         pronoun\n",
      "    PU         punctuation\n",
      "    SB         被 in short bei-const\n",
      "    VA         predicative adjective\n",
      "    VC         是 (copula)\n",
      "    VE         有 as the main verb\n",
      "    VV         other verb\n",
      "    NP         noun phrase\n",
      "    PP         prepositional phrase\n",
      "    VP         verb phrase\n",
      "    ADVP       adverb phrase\n",
      "    ADJP       adjective phrase\n",
      "    SBAR       subordinating conjunction\n",
      "    PRT        particle\n",
      "    PNP        prepositional noun phrase\n",
      "    acl        clausal modifier of noun (adjectival clause)\n",
      "    acomp      adjectival complement\n",
      "    advcl      adverbial clause modifier\n",
      "    advmod     adverbial modifier\n",
      "    agent      agent\n",
      "    amod       adjectival modifier\n",
      "    appos      appositional modifier\n",
      "    attr       attribute\n",
      "    aux        auxiliary\n",
      "    auxpass    auxiliary (passive)\n",
      "    case       case marking\n",
      "    cc         coordinating conjunction\n",
      "    ccomp      clausal complement\n",
      "    clf        classifier\n",
      "    complm     complementizer\n",
      "    compound   compound\n",
      "    conj       conjunct\n",
      "    cop        copula\n",
      "    csubj      clausal subject\n",
      "    csubjpass  clausal subject (passive)\n",
      "    dative     dative\n",
      "    dep        unclassified dependent\n",
      "    det        determiner\n",
      "    discourse  discourse element\n",
      "    dislocated dislocated elements\n",
      "    dobj       direct object\n",
      "    expl       expletive\n",
      "    fixed      fixed multiword expression\n",
      "    flat       flat multiword expression\n",
      "    goeswith   goes with\n",
      "    hmod       modifier in hyphenation\n",
      "    hyph       hyphen\n",
      "    infmod     infinitival modifier\n",
      "    intj       interjection\n",
      "    iobj       indirect object\n",
      "    list       list\n",
      "    mark       marker\n",
      "    meta       meta modifier\n",
      "    neg        negation modifier\n",
      "    nmod       modifier of nominal\n",
      "    nn         noun compound modifier\n",
      "    npadvmod   noun phrase as adverbial modifier\n",
      "    nsubj      nominal subject\n",
      "    nsubjpass  nominal subject (passive)\n",
      "    nounmod    modifier of nominal\n",
      "    npmod      noun phrase as adverbial modifier\n",
      "    num        number modifier\n",
      "    number     number compound modifier\n",
      "    nummod     numeric modifier\n",
      "    oprd       object predicate\n",
      "    obj        object\n",
      "    obl        oblique nominal\n",
      "    orphan     orphan\n",
      "    parataxis  parataxis\n",
      "    partmod    participal modifier\n",
      "    pcomp      complement of preposition\n",
      "    pobj       object of preposition\n",
      "    poss       possession modifier\n",
      "    possessive possessive modifier\n",
      "    preconj    pre-correlative conjunction\n",
      "    prep       prepositional modifier\n",
      "    prt        particle\n",
      "    punct      punctuation\n",
      "    quantmod   modifier of quantifier\n",
      "    rcmod      relative clause modifier\n",
      "    relcl      relative clause modifier\n",
      "    reparandum overridden disfluency\n",
      "    root       root\n",
      "    ROOT       root\n",
      "    vocative   vocative\n",
      "    xcomp      open clausal complement\n",
      "    ac         adpositional case marker\n",
      "    adc        adjective component\n",
      "    ag         genitive attribute\n",
      "    ams        measure argument of adjective\n",
      "    app        apposition\n",
      "    avc        adverbial phrase component\n",
      "    cd         coordinating conjunction\n",
      "    cj         conjunct\n",
      "    cm         comparative conjunction\n",
      "    cp         complementizer\n",
      "    cvc        collocational verb construction\n",
      "    da         dative\n",
      "    dh         discourse-level head\n",
      "    dm         discourse marker\n",
      "    ep         expletive es\n",
      "    hd         head\n",
      "    ju         junctor\n",
      "    mnr        postnominal modifier\n",
      "    mo         modifier\n",
      "    ng         negation\n",
      "    nk         noun kernel element\n",
      "    nmc        numerical component\n",
      "    oa         accusative object\n",
      "    oc         clausal object\n",
      "    og         genitive object\n",
      "    op         prepositional object\n",
      "    par        parenthetical element\n",
      "    pd         predicate\n",
      "    pg         phrasal genitive\n",
      "    ph         placeholder\n",
      "    pm         morphological particle\n",
      "    pnc        proper noun component\n",
      "    rc         relative clause\n",
      "    re         repeated element\n",
      "    rs         reported speech\n",
      "    sb         subject\n",
      "    sbp        passivized subject (PP)\n",
      "    sp         subject or predicate\n",
      "    svp        separable verb prefix\n",
      "    uc         unit component\n",
      "    vo         vocative\n",
      "    PERSON     People, including fictional\n",
      "    NORP       Nationalities or religious or political groups\n",
      "    FACILITY   Buildings, airports, highways, bridges, etc.\n",
      "    FAC        Buildings, airports, highways, bridges, etc.\n",
      "    ORG        Companies, agencies, institutions, etc.\n",
      "    GPE        Countries, cities, states\n",
      "    LOC        Non-GPE locations, mountain ranges, bodies of water\n",
      "    PRODUCT    Objects, vehicles, foods, etc. (not services)\n",
      "    EVENT      Named hurricanes, battles, wars, sports events, etc.\n",
      "    WORK_OF_ART Titles of books, songs, etc.\n",
      "    LAW        Named documents made into laws.\n",
      "    LANGUAGE   Any named language\n",
      "    DATE       Absolute or relative dates or periods\n",
      "    TIME       Times smaller than a day\n",
      "    PERCENT    Percentage, including \"%\"\n",
      "    MONEY      Monetary values, including unit\n",
      "    QUANTITY   Measurements, as of weight or distance\n",
      "    ORDINAL    \"first\", \"second\", etc.\n",
      "    CARDINAL   Numerals that do not fall under another type\n",
      "    PER        Named person or family.\n",
      "    MISC       Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "    EVT        Festivals, cultural events, sports events, weather phenomena, wars, etc.\n",
      "    PROD       Product, i.e. artificially produced entities including speeches, radio shows, programming languages, contracts, laws and ideas\n",
      "    DRV        Words (and phrases?) that are dervied from a name, but not a name in themselves, e.g. 'Oslo-mannen' ('the man from Oslo')\n",
      "    GPE_LOC    Geo-political entity, with a locative sense, e.g. 'John lives in Spain'\n",
      "    GPE_ORG    Geo-political entity, with an organisation sense, e.g. 'Spain declined to meet with Belgium'\n"
     ]
    }
   ],
   "source": [
    "token_labelling.explain_token_labels()"
>>>>>>> bf8ef79 (add notebook)
=======
>>>>>>> a71e2a8 (improve notebook explanation)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "### Batched token labelling\n",
=======
>>>>>>> bf8ef79 (add notebook)
=======
    "### Batched token labelling\n",
>>>>>>> a71e2a8 (improve notebook explanation)
    "Next, let us analyze a batch of sentences and have them labelled.\n",
    "> In the example below the input sentences are not yet tokenized, so spaCy uses its internal tokenizer."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 55,
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 18,
>>>>>>> a71e2a8 (improve notebook explanation)
=======
   "execution_count": 9,
>>>>>>> e0ed3b4 (add the files containing token information/labels)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Token: This\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | True        | False   | True       | False        | False   | False     | False          | False          | False            | False          \n",
=======
      "Token: Peter\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | True        | False        | False         | False     | False        | False                      | False         | False            | False   | False      | False       | False      | True           | False          | False                       | False     | False   | False    | True           \n",
>>>>>>> e0ed3b4 (add the files containing token information/labels)
      "---\n",
      "Token: is\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | True         | False                      | False         | False            | False   | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: a\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | True          | False            | False   | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: person\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | False         | False            | True    | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: .\n",
<<<<<<< HEAD
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False          \n",
=======
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | True        | False   | True       | False        | False   | False     | False          | False          | False            | True       | False     | False   | False        | False          \n",
=======
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | True        | False   | True       | False        | False   | False     | False          | False          | False            | False          \n",
>>>>>>> a71e2a8 (improve notebook explanation)
      "---\n",
      "Token: is\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | True    | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: a\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: sentence\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | True    | False      | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: .\n",
<<<<<<< HEAD
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False      | False     | False   | False        | False          \n",
>>>>>>> bf8ef79 (add notebook)
=======
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False          \n",
>>>>>>> a71e2a8 (improve notebook explanation)
=======
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | False         | False            | False   | False      | False       | False      | False          | True           | False                       | False     | False   | False    | False          \n",
>>>>>>> e0ed3b4 (add the files containing token information/labels)
      "---\n",
      "\n",
      "\n",
      "5\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': False, 'Is Pronoun': True, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': True, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
=======
      "[[False, True, False, True, False, False, False, False, False, False, True, False, False, False, False], [False, False, False, False, False, True, False, False, False, False, False, False, True, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, True, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n"
>>>>>>> bf8ef79 (add notebook)
=======
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': False, 'Is Pronoun': True, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': True, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
>>>>>>> a71e2a8 (improve notebook explanation)
=======
      "[{'Starts with space': False, 'Capitalized': True, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': True, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': True}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': True, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': True, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': True, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': True, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}]\n"
>>>>>>> e0ed3b4 (add the files containing token information/labels)
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Peter is a person.\"\n",
    "]\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "labels = token_labelling.label_batch_sentences(sentences, tokenized=False, verbose=True)\n",
=======
    "labels = token_labelling.label_batch_token(sentences, tokenized=False, verbose=True)\n",
>>>>>>> bf8ef79 (add notebook)
=======
    "labels = token_labelling.label_batch_sentences(sentences, tokenized=False, verbose=True)\n",
>>>>>>> a71e2a8 (improve notebook explanation)
    "\n",
    "print(len(labels[0]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our own tokenization. E.g. the one from our TinyStories models."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> bf8ef79 (add notebook)
=======
   "execution_count": 19,
>>>>>>> a71e2a8 (improve notebook explanation)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': True, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': True, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
=======
      "Token: This \n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | True        | True    | False      | False        | False   | False     | False          | False          | False            | False      | False     | True    | False        | False          \n",
      "---\n",
      "Token: is \n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | True      | False          | False          | False            | False      | False     | False   | False        | False          \n",
      "---\n",
      "Token: a \n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | False       | False   | False      | True         | False   | False     | False          | False          | False            | False      | False     | False   | False        | False          \n",
      "---\n",
      "Token: sentence\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | False       | True    | False      | False        | False   | False     | False          | False          | False            | False      | True      | False   | False        | False          \n",
      "---\n",
      "Token: .\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Subject | Is Object | Is Root | Is auxiliary | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False      | False     | False   | False        | False          \n",
      "---\n",
      "\n",
      "\n",
      "5\n",
      "[[False, True, True, False, False, False, False, False, False, False, False, False, True, False, False], [False, False, False, False, False, False, True, False, False, False, False, False, False, False, False], [False, False, False, False, True, False, False, False, False, False, False, False, False, False, False], [False, False, True, False, False, False, False, False, False, False, False, True, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n"
>>>>>>> bf8ef79 (add notebook)
=======
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': True, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': True, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
>>>>>>> a71e2a8 (improve notebook explanation)
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"This \", \"is \", \"a \", \"sentence\", \".\"]\n",
    "]\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "labelled_sentences = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "\n",
    "print(len(labelled_sentences[0]))\n",
    "print(labelled_sentences[0])"
=======
    "labels = token_labelling.label_batch_token(sentences, tokenized=True, verbose=False)\n",
    "\n",
    "print(len(labels[0]))\n",
    "print(labels[0])"
>>>>>>> bf8ef79 (add notebook)
=======
    "labelled_sentences = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "\n",
    "print(len(labelled_sentences[0]))\n",
    "print(labelled_sentences[0])"
>>>>>>> a71e2a8 (improve notebook explanation)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> a71e2a8 (improve notebook explanation)
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Labelling all tokens in the dataset\n",
    "\n",
    "Now we want to label all the tokens that our tokenizer knows - its entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is: 4096\n"
     ]
    }
   ],
   "source": [
    "# Get all the tokens of the tokenizer\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "def tokenize(tokenizer: PreTrainedTokenizer, sample_txt: str) -> list[int]:\n",
    "    # supposedly this can be different than prepending the bos token id\n",
    "    return tokenizer.encode(tokenizer.bos_token + sample_txt, return_tensors=\"pt\")[0]\n",
    "\n",
    "# Decode a sentence\n",
    "def decode(tokenizer: PreTrainedTokenizer, token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "model = \"delphi-suite/delphi-llama2-100k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"The vocab size is:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20: \u0011            21: \u0012            22: \u0013            23: \u0014            24: \u0015            25: \u0016            26: \u0017            27: \u0018            28: \u0019            29: \u001a          \n",
      " 800: te          801: happened    802: flow        803: food        804: list        805: just        806: Her         807: animals     808: hig         809: didn       \n",
      "1200: ice        1201: ount       1202: worked     1203: okay       1204: irt        1205: making     1206: dress      1207: enjoy      1208: advent     1209: bright     \n",
      "2300: lift       2301: ign        2302: ba         2303: line       2304: Doggy      2305: clouds     2306: dogs       2307: yard       2308: wolf       2309: spray      \n",
      "4086: 1          4087: 0          4088: 2          4089: 5          4090: 4          4091: 9          4092: 8          4093: 6          4094: 7          4095: $          \n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at some tokens\n",
    "ranges = [(20,30), (800,810), (1200,1210), (2300, 2310), (vocab_size-10, vocab_size)]\n",
    "for start, end in ranges:\n",
    "    for i in range(start, end):\n",
    "        print(f\"{i:4}:\",decode(tokenizer, i).ljust(10), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ff9bbe9364a3ea1ab9acc11abe338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labelling tokens:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's label each token\n",
    "labelled_token_ids_dict: dict[int, dict[str, bool]] = {}\n",
    "max_token_id = tokenizer.vocab_size  # stop at which token id, for testing, later vocab size\n",
    "batch_size = 500\n",
    "# we iterate (batchwise) over all token_ids\n",
    "for start in tqdm(range(0, max_token_id, batch_size), desc=\"Labelling tokens\"):\n",
    "    # create a batch of token_ids\n",
    "    end = min(start+batch_size, max_token_id)\n",
    "    token_ids = list(range(start, end))\n",
    "    # decode the token_ids to get a list of tokens, a 'sentence'\n",
    "    tokens = decode(tokenizer, token_ids)  # list of tokens == sentence\n",
    "    # put the sentence into a list, to make it a batch of sentences\n",
    "    sentences = [tokens]\n",
    "    # label the batch of sentences\n",
    "    labels = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "    # create a dict with the token_ids and their labels\n",
    "    labelled_sentence_dict = dict(zip(token_ids, labels[0]))\n",
    "    # update the labelled_token_ids_dict with the new dict\n",
    "    labelled_token_ids_dict.update(labelled_sentence_dict)\n",
    "    \n",
    "    \n",
    "    # print(token_ids)\n",
    "    # print(sentences)\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the dict (pickle it) for future reference.  \n",
    "With 4000 ids it currently takes ~150kB disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"labelled_token_ids_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labelled_token_ids_dict, f)"
   ]
<<<<<<< HEAD
=======
>>>>>>> bf8ef79 (add notebook)
=======
>>>>>>> a71e2a8 (improve notebook explanation)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tinyevals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< HEAD
   "version": "3.10.13"
=======
   "version": "3.8.8"
>>>>>>> bf8ef79 (add notebook)
=======
   "version": "3.10.13"
>>>>>>> a71e2a8 (improve notebook explanation)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
