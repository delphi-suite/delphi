{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving tokens a label - How to categorize tokens\n",
    "\n",
    "\n",
    "The first part of this Notebook contains elements that explain how to label tokens and how the functions work.\n",
    "\n",
    "The second part shows how all tokens are labelled that are used for our delphi language models.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import delphi\n",
    "\n",
    "from delphi.eval import token_labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1) How to use the token labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze a simple sentence and receive the respective tokens with their analyzed attributes.  \n",
    "The grammatical/linguistic analysis is done by a model provided by spaCy for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter \t PROPN \t nsubj \t PERSON\n",
      "is \t AUX \t ROOT \t \n",
      "a \t DET \t det \t \n",
      "person \t NOUN \t attr \t \n"
     ]
    }
   ],
   "source": [
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc object from a given text\n",
    "doc = nlp(\"Peter is a person\")\n",
    "\n",
    "token = doc[0]\n",
    "for tok in doc:\n",
    "    print(tok,\"\\t\", tok.pos_, \"\\t\", tok.dep_, \"\\t\", tok.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the label for our custom token that we just printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Capitalized': True,\n",
      " 'Is Adjective': False,\n",
      " 'Is Adposition': False,\n",
      " 'Is Adverb': False,\n",
      " 'Is Auxiliary': False,\n",
      " 'Is Coordinating conjuction': False,\n",
      " 'Is Determiner': False,\n",
      " 'Is Interjunction': False,\n",
      " 'Is Named Entity': True,\n",
      " 'Is Noun': False,\n",
      " 'Is Numeral': False,\n",
      " 'Is Other': False,\n",
      " 'Is Particle': False,\n",
      " 'Is Pronoun': False,\n",
      " 'Is Proper Noun': True,\n",
      " 'Is Punctuation': False,\n",
      " 'Is Subordinating conjuction': False,\n",
      " 'Is Symbol': False,\n",
      " 'Is Verb': False,\n",
      " 'Starts with space': False}\n"
     ]
    }
   ],
   "source": [
    "from delphi.eval import token_labelling\n",
    "\n",
    "label = token_labelling.label_single_token(token)\n",
    "pprint(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an understanding of what the labels acutally mean.\n",
    "Use this function to receive an explanation for a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Explanation of token labels --------\n",
      "Token text:          Peter\n",
      "Token dependency:    nominal subject\n",
      "Token POS:           proper noun\n",
      "---------------- Token labels ---------------\n",
      "  0   Starts with space    False\n",
      "  1   Capitalized          True\n",
      "  2   Is Adjective         False\n",
      "  3   Is Adposition        False\n",
      "  4   Is Adverb            False\n",
      "  5   Is Auxiliary         False\n",
      "  6   Is Coordinating conjuction False\n",
      "  7   Is Determiner        False\n",
      "  8   Is Interjunction     False\n",
      "  9   Is Noun              False\n",
      " 10   Is Numeral           False\n",
      " 11   Is Particle          False\n",
      " 12   Is Pronoun           False\n",
      " 13   Is Proper Noun       True\n",
      " 14   Is Punctuation       False\n",
      " 15   Is Subordinating conjuction False\n",
      " 16   Is Symbol            False\n",
      " 17   Is Verb              False\n",
      " 18   Is Other             False\n",
      " 19   Is Named Entity      True\n"
     ]
    }
   ],
   "source": [
    "token_labelling.explain_token_labels(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in all the possible labels a token can have, that spaCy is capable of assigning, then call the same function but without any argument:\n",
    "```Python\n",
    ">>> token_labelling.explain_token_labels()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched token labelling\n",
    "Next, let us analyze a batch of sentences and have them labelled.\n",
    "> In the example below the input sentences are not yet tokenized, so spaCy uses its internal tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Peter\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | True        | False        | False         | False     | False        | False                      | False         | False            | False   | False      | False       | False      | True           | False          | False                       | False     | False   | False    | True           \n",
      "---\n",
      "Token: is\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | True         | False                      | False         | False            | False   | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: a\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | True          | False            | False   | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: person\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | False         | False            | True    | False      | False       | False      | False          | False          | False                       | False     | False   | False    | False          \n",
      "---\n",
      "Token: .\n",
      "Starts with space | Capitalized | Is Adjective | Is Adposition | Is Adverb | Is Auxiliary | Is Coordinating conjuction | Is Determiner | Is Interjunction | Is Noun | Is Numeral | Is Particle | Is Pronoun | Is Proper Noun | Is Punctuation | Is Subordinating conjuction | Is Symbol | Is Verb | Is Other | Is Named Entity\n",
      "False             | False       | False        | False         | False     | False        | False                      | False         | False            | False   | False      | False       | False      | False          | True           | False                       | False     | False   | False    | False          \n",
      "---\n",
      "\n",
      "\n",
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': True, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': True}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': True, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': True, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': True, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': False, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Adjective': False, 'Is Adposition': False, 'Is Adverb': False, 'Is Auxiliary': False, 'Is Coordinating conjuction': False, 'Is Determiner': False, 'Is Interjunction': False, 'Is Noun': False, 'Is Numeral': False, 'Is Particle': False, 'Is Pronoun': False, 'Is Proper Noun': False, 'Is Punctuation': True, 'Is Subordinating conjuction': False, 'Is Symbol': False, 'Is Verb': False, 'Is Other': False, 'Is Named Entity': False}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Peter is a person.\"\n",
    "]\n",
    "labels = token_labelling.label_batch_sentences(sentences, tokenized=False, verbose=True)\n",
    "\n",
    "print(len(labels[0]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our own tokenization. E.g. the one from our TinyStories models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': True, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': True, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"This \", \"is \", \"a \", \"sentence\", \".\"]\n",
    "]\n",
    "labelled_sentences = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "\n",
    "print(len(labelled_sentences[0]))\n",
    "print(labelled_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Labelling all tokens in the dataset\n",
    "\n",
    "Now we want to label all the tokens that our tokenizer knows - its entire vocabulary.\n",
    "\n",
    "Using thy script in `scripts/label_all_tokens.py` we get the files:\n",
    "- `src\\delphi\\eval\\all_tokens_list.txt`\n",
    "- `src\\delphi\\eval\\labelled_token_ids_dict.pkl`\n",
    "\n",
    "Let's load the tokenizer so that we can look at the labelled tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshu\\anaconda3\\envs\\delphi2\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is: 4096\n"
     ]
    }
   ],
   "source": [
    "# Get all the tokens of the tokenizer\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "# Decode a sentence\n",
    "def decode(tokenizer: PreTrainedTokenizer, token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "model = \"delphi-suite/delphi-llama2-100k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"The vocab size is:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = \"../src/delphi/eval/labelled_token_ids_dict.pkl\"\n",
    "# load \n",
    "with open(path, \"rb\") as f:\n",
    "    labelled_token_ids_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some random tokens and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token id is: 1143\n",
      "The decoded token is: has\n",
      "The label is:\n",
      "{'Capitalized': False,\n",
      " 'Is Adjective': False,\n",
      " 'Is Adposition': False,\n",
      " 'Is Adverb': False,\n",
      " 'Is Auxiliary': False,\n",
      " 'Is Coordinating conjuction': False,\n",
      " 'Is Determiner': False,\n",
      " 'Is Interjunction': True,\n",
      " 'Is Named Entity': False,\n",
      " 'Is Noun': False,\n",
      " 'Is Numeral': False,\n",
      " 'Is Other': False,\n",
      " 'Is Particle': False,\n",
      " 'Is Pronoun': False,\n",
      " 'Is Proper Noun': False,\n",
      " 'Is Punctuation': False,\n",
      " 'Is Subordinating conjuction': False,\n",
      " 'Is Symbol': False,\n",
      " 'Is Verb': False,\n",
      " 'Starts with space': False}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "# Get a random token id between 0 and 4000\n",
    "token_id = random.randint(0, 4000)\n",
    "# decode the token id\n",
    "decoded_token = decode(tokenizer, [token_id])\n",
    "# get the corresponding label\n",
    "label = labelled_token_ids_dict[token_id]\n",
    "# print the results\n",
    "print(\"The token id is:\", token_id)\n",
    "print(\"The decoded token is:\", decoded_token)\n",
    "print(\"The label is:\")\n",
    "pprint(label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tinyevals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
