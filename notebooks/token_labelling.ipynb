{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving tokens a label - How to categorize tokens\n",
    "\n",
    "\n",
    "The first part of this Notebook contains elements that explain how to label tokens and how the functions work.\n",
    "\n",
    "The second part shows how all tokens are labelled that are used for our delphi language models.3\n",
    "\n",
    "# 1) How to use the token labelling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import delphi\n",
    "\n",
    "# from delphi.eval import token_labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze a simple sentence and receive the respective tokens with their analyzed attributes.  \n",
    "The grammatical/linguistic analysis is done by a model provided by spaCy for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n"
     ]
    }
   ],
   "source": [
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc object from a given text\n",
    "doc = nlp(\"This is a dummy sentence for testing.\")\n",
    "\n",
    "token = doc[0]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the label for our custom token that we just printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "{'Capitalized': True,\n",
      " 'Is Adjective': False,\n",
      " 'Is Adverb': False,\n",
      " 'Is Conjunction': False,\n",
      " 'Is Interjunction': False,\n",
      " 'Is Named Entity': False,\n",
      " 'Is Noun': False,\n",
      " 'Is Preposition': False,\n",
      " 'Is Pronoun': True,\n",
      " 'Is Verb': False,\n",
      " 'Starts with space': False}\n"
     ]
    }
   ],
   "source": [
    "from delphi.eval import token_labelling\n",
    "\n",
    "label = token_labelling.label_single_token(token)\n",
    "pprint(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an understanding of what the labels acutally mean.\n",
    "Use this function to receive an explanation for a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Explanation of token labels --------\n",
      "Token text:          This\n",
      "Token dependency:    nominal subject\n",
      "Token POS:           pronoun\n",
      "---------------- Token labels ---------------\n",
      "  0   Starts with space    False\n",
      "  1   Capitalized          True\n",
      "  2   Is Noun              False\n",
      "  3   Is Pronoun           True\n",
      "  4   Is Adjective         False\n",
      "  5   Is Verb              False\n",
      "  6   Is Adverb            False\n",
      "  7   Is Preposition       False\n",
      "  8   Is Conjunction       False\n",
      "  9   Is Interjunction     False\n",
      " 10   Is Named Entity      False\n"
     ]
    }
   ],
   "source": [
    "token_labelling.explain_token_labels(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in all the possible labels a token can have, that spaCy is capable of assigning, then call the same function but without any argument:\n",
    "```Python\n",
    ">>> token_labelling.explain_token_labels()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched token labelling\n",
    "Next, let us analyze a batch of sentences and have them labelled.\n",
    "> In this example the input sentences are not yet tokenized, so spaCy uses its internal tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: This\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | True        | False   | True       | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: is\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | True    | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: a\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: sentence\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | True    | False      | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "Token: .\n",
      "Starts with space | Capitalized | Is Noun | Is Pronoun | Is Adjective | Is Verb | Is Adverb | Is Preposition | Is Conjunction | Is Interjunction | Is Named Entity\n",
      "False             | False       | False   | False      | False        | False   | False     | False          | False          | False            | False          \n",
      "---\n",
      "\n",
      "\n",
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': False, 'Is Pronoun': True, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': True, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This is a sentence.\"\n",
    "]\n",
    "labels = token_labelling.label_batch_sentences(sentences, tokenized=False, verbose=True)\n",
    "\n",
    "print(len(labels[0]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our own tokenization. E.g. the one from our TinyStories models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[{'Starts with space': False, 'Capitalized': True, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': True, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': True, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': True, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}, {'Starts with space': False, 'Capitalized': False, 'Is Noun': False, 'Is Pronoun': False, 'Is Adjective': False, 'Is Verb': False, 'Is Adverb': False, 'Is Preposition': False, 'Is Conjunction': False, 'Is Interjunction': False, 'Is Named Entity': False}]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"This \", \"is \", \"a \", \"sentence\", \".\"]\n",
    "]\n",
    "labelled_sentences = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "\n",
    "print(len(labelled_sentences[0]))\n",
    "print(labelled_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Labelling all tokens in the dataset\n",
    "\n",
    "Now we want to label all the tokens that our tokenizer knows - its entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is  50257\n"
     ]
    }
   ],
   "source": [
    "# Get all the tokens of the tokenizer\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "def tokenize(tokenizer: PreTrainedTokenizer, sample_txt: str) -> list[int]:\n",
    "    # supposedly this can be different than prepending the bos token id\n",
    "    return tokenizer.encode(tokenizer.bos_token + sample_txt, return_tensors=\"pt\")[0]\n",
    "\n",
    "# Decode a sentence\n",
    "def decode(tokenizer: PreTrainedTokenizer, token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"The vocab size is:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!          \"          #          $          %          &          '          (          )          *          \n",
      " inv       lect        supp      ating       look      man        pect        8         row         bu        \n",
      " child      since     ired       less        life       develop   ittle       dep        pass      �          \n",
      " matter    reg        ext        angu       isc        ole        aut         compet    eed        fect       \n",
      " (/        ….\"        Compar      amplification ominated    regress    Collider   informants  gazed                \n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at some tokens\n",
    "ranges = [(0,10), (800,810), (1200,1210), (2300, 2310), (vocab_size-10, vocab_size)]\n",
    "for start, end in ranges:\n",
    "    for i in range(start, end):\n",
    "        print(decode(tokenizer, i).ljust(10), end=\" \")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2405771500d24b7890f87694d533486f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labelling tokens:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's label each token\n",
    "labelled_token_ids_dict: dict[int, dict[str, bool]] = {}\n",
    "max_token_id = 4000  # stop at which token id, for testing, later vocab size\n",
    "batch_size = 500\n",
    "# we iterate (batchwise) over all token_ids\n",
    "for start in tqdm(range(0, max_token_id, batch_size), desc=\"Labelling tokens\"):\n",
    "    # create a batch of token_ids\n",
    "    token_ids = list(range(start, start+batch_size))\n",
    "    # decode the token_ids to get a list of tokens, a 'sentence'\n",
    "    tokens = decode(tokenizer, token_ids)  # list of tokens == sentence\n",
    "    # put the sentence into a list, to make it a batch of sentences\n",
    "    sentences = [tokens]\n",
    "    # label the batch of sentences\n",
    "    labels = token_labelling.label_batch_sentences(sentences, tokenized=True, verbose=False)\n",
    "    # create a dict with the token_ids and their labels\n",
    "    labelled_sentence_dict = dict(zip(token_ids, labels[0]))\n",
    "    # update the labelled_token_ids_dict with the new dict\n",
    "    labelled_token_ids_dict.update(labelled_sentence_dict)\n",
    "    \n",
    "    # print(token_ids)\n",
    "    # print(sentences)\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the dict (pickle it) for future reference.  \n",
    "With 4000 ids it currently takes ~150kB disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"labelled_token_ids_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labelled_token_ids_dict, f)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tinyevals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
