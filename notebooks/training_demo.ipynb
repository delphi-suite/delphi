{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 512\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 1376\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 16\n",
      "Setting llama2hf_config.num_hidden_layers to 8\n",
      "Setting llama2hf_config.num_key_value_heads to 8\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.35.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 168\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 448\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 12\n",
      "Setting llama2hf_config.num_hidden_layers to 6\n",
      "Setting llama2hf_config.num_key_value_heads to 6\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.35.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.attention_dropout to 0.0\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 120\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 320\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 12\n",
      "Setting llama2hf_config.num_hidden_layers to 6\n",
      "Setting llama2hf_config.num_key_value_heads to 6\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.36.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.attention_dropout to 0.0\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 48\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 128\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 8\n",
      "Setting llama2hf_config.num_hidden_layers to 4\n",
      "Setting llama2hf_config.num_key_value_heads to 4\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.36.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting architecture to llama2-huggingface\n",
      "Setting vocab_size to 4096\n",
      "Setting max_seq_len to 512\n",
      "Setting dim to 48\n",
      "Setting n_layers to 2\n",
      "Setting n_heads to 2\n",
      "Setting n_kv_heads to 2\n",
      "Setting max_epochs to 2\n",
      "Setting eval_interval to 500\n",
      "Setting eval_iters to 10\n",
      "Setting train_sample_limit to 256\n",
      "Setting llama2hf_config.hidden_size to 48\n",
      "Setting llama2hf_config.intermediate_size to 48\n",
      "Setting llama2hf_config.num_attention_heads to 2\n",
      "Setting llama2hf_config.num_hidden_layers to 2\n",
      "Setting llama2hf_config.num_key_value_heads to 2\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 216\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 576\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 12\n",
      "Setting llama2hf_config.num_hidden_layers to 6\n",
      "Setting llama2hf_config.num_key_value_heads to 6\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.35.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.attention_dropout to 0.0\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 64\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 192\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 8\n",
      "Setting llama2hf_config.num_hidden_layers to 4\n",
      "Setting llama2hf_config.num_key_value_heads to 4\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.36.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.attention_dropout to 0.0\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 256\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 704\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 16\n",
      "Setting llama2hf_config.num_hidden_layers to 8\n",
      "Setting llama2hf_config.num_key_value_heads to 8\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.36.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 384\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 1024\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 16\n",
      "Setting llama2hf_config.num_hidden_layers to 8\n",
      "Setting llama2hf_config.num_key_value_heads to 8\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.35.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Setting llama2hf_config.attention_bias to False\n",
      "Setting llama2hf_config.attention_dropout to 0.0\n",
      "Setting llama2hf_config.bos_token_id to 1\n",
      "Setting llama2hf_config.eos_token_id to 2\n",
      "Setting llama2hf_config.hidden_act to silu\n",
      "Setting llama2hf_config.hidden_size to 96\n",
      "Setting llama2hf_config.initializer_range to 0.02\n",
      "Setting llama2hf_config.intermediate_size to 256\n",
      "Setting llama2hf_config.max_position_embeddings to 512\n",
      "Setting llama2hf_config.model_type to llama\n",
      "Setting llama2hf_config.num_attention_heads to 8\n",
      "Setting llama2hf_config.num_hidden_layers to 4\n",
      "Setting llama2hf_config.num_key_value_heads to 4\n",
      "Setting llama2hf_config.pretraining_tp to 1\n",
      "Setting llama2hf_config.rms_norm_eps to 1e-05\n",
      "Setting llama2hf_config.rope_theta to 10000.0\n",
      "Setting llama2hf_config.tie_word_embeddings to True\n",
      "Setting llama2hf_config.transformers_version to 4.36.2\n",
      "Setting llama2hf_config.use_cache to True\n",
      "Setting llama2hf_config.vocab_size to 4096\n",
      "Starting training...\n",
      "\n",
      "Config:\n",
      "  device: auto\n",
      "  architecture: llama2-huggingface\n",
      "  out_dir: out\n",
      "  eval_interval: 500\n",
      "  log_interval: 1\n",
      "  eval_iters: 10\n",
      "  eval_only: False\n",
      "  always_save_checkpoint: False\n",
      "  init_from: scratch\n",
      "  wandb_log: False\n",
      "  wandb_entity: jaiwithani\n",
      "  wandb_project: delphi\n",
      "  wandb_run_name: 2024_03_11_12_41_12\n",
      "  batch_size: 64\n",
      "  dim: 48\n",
      "  max_seq_len: 512\n",
      "  vocab_size: 4096\n",
      "  n_layers: 2\n",
      "  n_heads: 2\n",
      "  n_kv_heads: 2\n",
      "  multiple_of: 32\n",
      "  dropout: 0.0\n",
      "  llama2hf_config: Llama2ConfigData(attention_bias=False, attention_dropout=0.0, bos_token_id=-1, eos_token_id=-2, hidden_act='silu', hidden_size=48, initializer_range=0.02, intermediate_size=48, max_position_embeddings=513, model_type='llama', num_attention_heads=2, num_hidden_layers=2, num_key_value_heads=2, pretraining_tp=1, rms_norm_eps=1e-06, rope_scaling=None, rope_theta=10000.0, tie_word_embeddings=False, transformers_version='4.36.2', use_cache=True, vocab_size=4096)\n",
      "  gradient_accumulation_steps: 4\n",
      "  learning_rate: 0.0005\n",
      "  max_epochs: 2\n",
      "  weight_decay: 0.1\n",
      "  beta1: 0.9\n",
      "  beta2: 0.95\n",
      "  grad_clip: 1.0\n",
      "  decay_lr: True\n",
      "  warmup_iters: 1000\n",
      "  min_lr: 0.0\n",
      "  train_sample_limit: 256\n",
      "  val_sample_limit: -1\n",
      "Device: mps\n",
      "Loading data...\n",
      "tokens per iteration will be: 131,072\n",
      "breaks down as: 4 grad accum steps * 64 batch size * 512 max seq len\n",
      "Setting up...\n",
      "Initializing a new model from scratch\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7745, val loss 8.3234\n",
      "gradient accumulation steps: 4, num_steps: 1, iter_num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | loss 8.3235 | lr 0.000000e+00 | 6880.43ms | mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient accumulation steps: 4, num_steps: 1, iter_num: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | loss 8.3241 | lr 5.000000e-07 | 1959.11ms | mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from delphi.train.config_utils import get_presets_by_name\n",
    "from delphi.train.training import run_training\n",
    "from delphi.train.utils import ModelTrainingState\n",
    "\n",
    "\n",
    "def train() -> ModelTrainingState:\n",
    "    config = get_presets_by_name()[\"v0-llama2-100k\"]\n",
    "    config.wandb_entity = \"jaiwithani\"\n",
    "    return run_training(config)\n",
    "\n",
    "model_train_result = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyevals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
