{
    "architecture": "llama2-huggingface",
    "vocab_size": 4096,
    "max_seq_len": 512,
    "dim": 48,
    "n_layers": 2,
    "n_heads": 2,
    "n_kv_heads": 2,
    "max_epochs": 2,
    "eval_interval": 500,
    "eval_iters": 10,
    "train_sample_limit": 256,
    "llama2hf_config": {
        "hidden_size": 48,
        "intermediate_size": 48,
        "num_attention_heads": 2,
        "num_hidden_layers": 2,
        "num_key_value_heads": 2
    }
}