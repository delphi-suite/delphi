{
    "priority": -1,
    "architecture": "llama",
    "vocab_size": 4096,
    "max_seq_len": 512,
    "max_epochs": 2,
    "eval_interval": 1,
    "eval_iters": 1,
    "train_sample_limit": 256,
    "batch_size": 64,
    "model_config": {
        "model_type": "llama",
        "llama": {
            "hidden_size": 48,
            "intermediate_size": 48,
            "num_attention_heads": 2,
            "num_hidden_layers": 2,
            "num_key_value_heads": 2,
            "vocab_size": 4096
        }
    }
}