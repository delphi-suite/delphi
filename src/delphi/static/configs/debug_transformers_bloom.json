{
    "priority": -1,
    "vocab_size": 4096,
    "max_seq_len": 512,
    "max_epochs": 2,
    "eval_interval": 1,
    "eval_iters": 1,
    "data_config": {
        "train_sample_limit": 256
    },
    "batch_size": 64,
    "model_config": {
        "model_class": "BloomForCausalLM",
        "apply_residual_connection_post_layernorm": false,
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_dropout": 0.0,
        "hidden_size": 8,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "n_head": 2,
        "n_layer": 2,
        "pretraining_tp": 1,
        "slow_but_exact": false,
        "use_cache": true,
        "vocab_size": 4096
    }
}