{
    "max_seq_len": 512,
    "max_epochs": 10,
    "eval_iters": 8,
    "batch_size": 64,
    "model_config": {
        "model_class": "BloomForCausalLM",
        "apply_residual_connection_post_layernorm": false,
        "attention_dropout": 0.0,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_dropout": 0.0,
        "hidden_size": 64,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "n_head": 8,
        "n_layer": 10,
        "pretraining_tp": 1,
        "slow_but_exact": false,
        "use_cache": true,
        "vocab_size": 4096
    }
}