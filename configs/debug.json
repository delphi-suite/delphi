{
    "max_seq_len": 512,
    "max_epochs": 2,
    "eval_iters": 1,
    "batch_ordering_seed": 42,
    "torch_seed": 1337,
    "batch_size": 64,
    "model_config": {
        "model_class": "LlamaForCausalLM",
        "hidden_size": 48,
        "intermediate_size": 48,
        "num_attention_heads": 2,
        "num_hidden_layers": 2,
        "num_key_value_heads": 2,
        "vocab_size": 4096
    },
    "dataset": {
        "name": "delphi-suite/v0-tinystories-v2-clean-tokenized"
    }
}