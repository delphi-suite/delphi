{
  "max_seq_len": 512,
  "max_epochs": 2,
  "log_interval": 1,
  "eval_iters": 10,
  "batch_size": 8,
  "model_config": {
    "model_class": "MambaForCausalLM",
    "vocab_size": 4096,
    "hidden_size": 48,
    "state_size": 16,
    "num_hidden_layers": 2,
    "conv_kernel": 2,
    "expand": 2,
    "time_step_rank": 2
  },
  "batch_ordering_seed": 42,
  "torch_seed": 1337,
  "dataset": {
    "name": "delphi-suite/v0-tinystories-v2-clean-tokenized"
  }
}