{
    "model_config": {
        "model_class": "MambaForCausalLM",
        "vocab_size": 4096,
        "state_size": 16,
        "layer_norm_epsilon": 1e-5,
        "bos_token_id": 0,
        "eos_token_id": 1,
        "pad_token_id": 2,
        "expand": 2,
        "conv_kernel": 4,
        "use_bias": false,
        "use_conv_bias": true,
        "hidden_act": "silu",
        "initializer_range": 0.1,
        "residual_in_fp32": true,
        "rescale_prenorm_residual": true
    },
    "max_seq_len": 512,
    "device": "auto",
    "checkpoint_interval": 400,
    "extra_checkpoint_iters": [
        1,
        2,
        4,
        8,
        16,
        32,
        64,
        128,
        256,
        512
    ],
    "log_interval": 40,
    "eval_iters": 10,
    "batch_size": 256,
    "max_epochs": 10,
    "grad_clip": 1.0,
    "gradient_accumulation_steps": 1,
    "adam": {
        "learning_rate": 0.0005,
        "weight_decay": 0.1,
        "beta1": 0.9,
        "beta2": 0.95,
        "decay_lr": true,
        "warmup_iters": 1000,
        "min_lr": 0.0
    },
    "batch_ordering_seed": 1337,
    "torch_seed": 42,
    "dataset": {
        "name": "delphi-suite/stories-tokenized"
    }
}