{
    "model_config": {
        "model_class": "LlamaForCausalLM",
        "vocab_size": 4096,
        "hidden_act": "silu",
        "max_position_embeddings": 512,
        "initializer_range": 0.02,
        "rms_norm_eps": 1e-06,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "tie_word_embeddings": false,
        "rope_theta": 10000.0,
        "rope_scaling": null,
        "attention_bias": false,
        "attention_dropout": 0.0
    },
    "max_seq_len": 512,
    "device": "auto",
    "checkpoint_interval": -1000,
    "extra_checkpoint_iters": [
        -1000,
        -2000
    ],
    "log_interval": -1000,
    "eval_iters": 100,
    "batch_size": 256,
    "max_epochs": 10,
    "grad_clip": 1.0,
    "adam": {
        "gradient_accumulation_steps": 1,
        "learning_rate": 0.0005,
        "weight_decay": 0.1,
        "beta1": 0.9,
        "beta2": 0.95,
        "decay_lr": true,
        "warmup_iters": 1000,
        "min_lr": 0.0
    },
    "batch_ordering_seed": 1337,
    "torch_seed": 42,
    "dataset": {
        "name": "delphi-suite/stories-tokenized"
    }
}